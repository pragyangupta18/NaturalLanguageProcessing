{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Natural Language - Text Processing"
      ],
      "metadata": {
        "id": "dYwcNcZf4apz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "9gcrx6mQ3vR4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize , sent_tokenize\n",
        "\n",
        "text = \"I believe this would help the reader understand how tokenization \\\n",
        "works. as well as realize its importance.\"\n"
      ],
      "metadata": {
        "id": "uIhi3mnm3w8H",
        "outputId": "5f7057ff-5e8f-4414-b54f-3cdb787c08f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sents = (sent_tokenize(text))\n",
        "print(sents)"
      ],
      "metadata": {
        "id": "G4TG6dwn4YN5",
        "outputId": "9a8c356f-1e79-4fe6-c98c-f930b49b59f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I believe this would help the reader understand how tokenization works.', 'as well as realize its importance.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = (word_tokenize(text))\n",
        "print(words)"
      ],
      "metadata": {
        "id": "5ttg3-nf5sgZ",
        "outputId": "1075a91b-3cc0-48a1-88a2-b80423ee3850",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'believe', 'this', 'would', 'help', 'the', 'reader', 'understand', 'how', 'tokenization', 'works', '.', 'as', 'well', 'as', 'realize', 'its', 'importance', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = [word_tokenize(sent) for sent in sents]\n",
        "print (words)"
      ],
      "metadata": {
        "id": "sBN1kl0x53TT",
        "outputId": "f77dc352-0678-4708-97e8-bce9bcd02bd2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['I', 'believe', 'this', 'would', 'help', 'the', 'reader', 'understand', 'how', 'tokenization', 'works', '.'], ['as', 'well', 'as', 'realize', 'its', 'importance', '.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stop Word"
      ],
      "metadata": {
        "id": "4rI2fGZ57ldg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import word_tokenize , sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from string import punctuation\n",
        "text = \"I believe this would help the reader understand how tokenization \\\n",
        "works. as well as realize its importance (text).\"\n"
      ],
      "metadata": {
        "id": "l0ghiwbY7nVU",
        "outputId": "28c7d39b-5f25-4ff5-a850-ec8bf02f104f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(punctuation)\n",
        "# These are the punctuations available"
      ],
      "metadata": {
        "id": "FGlvFrLj8PpP",
        "outputId": "02954291-57ba-4901-d937-1f2a66a6afce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "custom_list = set(stopwords.words('english')+list(punctuation))\n",
        "print(custom_list)"
      ],
      "metadata": {
        "id": "fXuUxwTX8Uhf",
        "outputId": "1a7e47e3-82e3-418b-8a8c-784eb0db4073",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'by', 'hers', 'yourself', \"you're\", 'won', 'her', 't', 'this', 'o', 'he', 've', 'can', 'needn', 'an', 'hasn', '/', \"she's\", 'she', 'do', 'as', 'not', 'be', 'will', 'after', 'other', 'on', 'further', '<', 'off', 'should', 'so', \"aren't\", 'doesn', 'now', 'hadn', 'for', 'with', 'while', 'some', 'y', 'who', 'haven', 'mustn', \"'\", 'ain', 'very', '^', '@', ';', '>', 'those', 'me', 'through', 'until', '}', 'the', 'ours', \"didn't\", 'shan', 'if', \"hasn't\", \"should've\", 'against', 'has', 'my', 'where', 'just', 'm', 'theirs', \"couldn't\", 'been', \"shouldn't\", '*', 'ma', 'don', 's', '-', 'any', '(', 'had', '$', 'couldn', 'does', 'again', 'doing', 'i', 'isn', ']', 'weren', \"you'll\", 'of', 'at', 'same', \"needn't\", 'or', ')', 'which', 'himself', \"isn't\", 'its', 'here', 'few', 'to', '%', 'myself', 'but', 'down', \"mightn't\", 'each', 'too', \"mustn't\", ',', 'below', 'how', 'have', 'their', 'in', 'over', '#', 'our', 'and', 'such', \"hadn't\", 'out', '_', 'both', 'up', 'from', 'ourselves', 'herself', 'a', 'd', \"you've\", \"doesn't\", \"shan't\", 'own', 'him', '|', 'during', \"that'll\", \"it's\", \"wasn't\", 'there', 'were', \"you'd\", 'why', 're', '!', 'did', 'because', \"wouldn't\", 'll', 'mightn', 'they', 'you', 'yourselves', 'itself', 'are', 'above', 'wouldn', '?', 'before', 'whom', 'is', 'when', 'your', 'once', '.', '&', '~', '{', \"haven't\", '`', 'themselves', \"don't\", 'that', 'didn', '\\\\', 'into', 'aren', 'about', \"weren't\", '\"', ':', 'being', 'most', 'having', 'all', 'under', \"won't\", 'yours', 'what', 'these', 'no', 'we', 'than', '+', 'only', 'it', '[', 'them', 'was', 'then', 'nor', 'between', 'am', 'shouldn', 'his', '=', 'wasn', 'more'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_list = [word for word in word_tokenize(text) if word not in custom_list ]\n",
        "print(word_list)"
      ],
      "metadata": {
        "id": "xwNvwsIb-aN-",
        "outputId": "16a5c735-d761-4f0e-f600-feaec3772230",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'believe', 'would', 'help', 'reader', 'understand', 'tokenization', 'works', 'well', 'realize', 'importance', 'text']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# N-Grams"
      ],
      "metadata": {
        "id": "rp_EZ1amCjeC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bigram"
      ],
      "metadata": {
        "id": "F_ETx3A8GXj0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.collocations import BigramCollocationFinder\n",
        "\n",
        "word_list = ['I', 'believe', 'would', 'help', 'reader', 'understand', 'tokenization', 'works', 'well', 'realize', 'importance', 'text']"
      ],
      "metadata": {
        "id": "dlUJ8oDpCmL-"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finder = BigramCollocationFinder.from_words(word_list)\n",
        "print(finder.ngram_fd.items())"
      ],
      "metadata": {
        "id": "3oHa93IEEmdF",
        "outputId": "89dffff6-3ceb-438e-993b-8bb1056ba569",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_items([(('I', 'believe'), 1), (('believe', 'would'), 1), (('would', 'help'), 1), (('help', 'reader'), 1), (('reader', 'understand'), 1), (('understand', 'tokenization'), 1), (('tokenization', 'works'), 1), (('works', 'well'), 1), (('well', 'realize'), 1), (('realize', 'importance'), 1), (('importance', 'text'), 1)])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Trigram"
      ],
      "metadata": {
        "id": "EiEEW0h6GYzQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.collocations import TrigramCollocationFinder\n",
        "\n",
        "word_list = ['I', 'believe', 'would', 'help', 'reader', 'understand', 'tokenization', 'works', 'well', 'realize', 'importance', 'text']"
      ],
      "metadata": {
        "id": "UPXr8E-GGZz4"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finder = TrigramCollocationFinder.from_words(word_list)\n",
        "print(finder.ngram_fd.items())"
      ],
      "metadata": {
        "id": "YM2yHCrHGjqk",
        "outputId": "8924ea92-f0f5-45f5-aec8-a5b8aea8438c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dict_items([(('I', 'believe', 'would'), 1), (('believe', 'would', 'help'), 1), (('would', 'help', 'reader'), 1), (('help', 'reader', 'understand'), 1), (('reader', 'understand', 'tokenization'), 1), (('understand', 'tokenization', 'works'), 1), (('tokenization', 'works', 'well'), 1), (('works', 'well', 'realize'), 1), (('well', 'realize', 'importance'), 1), (('realize', 'importance', 'text'), 1)])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stemming"
      ],
      "metadata": {
        "id": "O_gKqsItGtL8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "\n",
        "new_text = \"It is important to by very pythonly while you are pythoning) with python. All pythoners have pythoned poorly at least once]\"\n"
      ],
      "metadata": {
        "id": "2svvSgvDGxw9"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "l_s = LancasterStemmer()\n",
        "stem_lan = [l_s.stem(word)for word in word_tokenize(new_text)]\n",
        "print(stem_lan)"
      ],
      "metadata": {
        "id": "iUTCfApNdDwj",
        "outputId": "b086b68c-a0cc-4484-b317-171c2f0c59f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['it', 'is', 'import', 'to', 'by', 'very', 'python', 'whil', 'you', 'ar', 'python', ')', 'with', 'python', '.', 'al', 'python', 'hav', 'python', 'poor', 'at', 'least', 'ont', ']']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# WSD- Word Sense Disambiguation"
      ],
      "metadata": {
        "id": "w5Qtl1U8gJtF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "L_XJKONNgP3Y",
        "outputId": "0470e992-e611-4ec3-c34d-653bcb5096a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for ss in wordnet.synsets('mouse') :\n",
        "  print(ss , ss.definition())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGJ0cXGZd6NH",
        "outputId": "006d9034-32c4-47d0-baa4-2f7bcad8bf38"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synset('mouse.n.01') any of numerous small rodents typically resembling diminutive rats having pointed snouts and small ears on elongated bodies with slender usually hairless tails\n",
            "Synset('shiner.n.01') a swollen bruise caused by a blow to the eye\n",
            "Synset('mouse.n.03') person who is quiet or timid\n",
            "Synset('mouse.n.04') a hand-operated electronic device that controls the coordinates of a cursor on your computer screen as you move it around on a pad; on the bottom of the device is a ball that rolls on the surface of the pad\n",
            "Synset('sneak.v.01') to go stealthily or furtively\n",
            "Synset('mouse.v.02') manipulate the mouse of a computer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.wsd import lesk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "context_1 = lesk(word_tokenize(\"Sing in a lower tone, along with the bass\"), \"bass\")\n",
        "print(context_1, context_1.definition())\n",
        "\n",
        "context_2 = lesk(word_tokenize(\"The sea bass is really very hard to catch\"), \"bass\")\n",
        "print(context_2, context_2.definition())\n"
      ],
      "metadata": {
        "id": "-W3ytpx-FKto",
        "outputId": "139d2f48-f7e5-448c-c21c-954d01995b2b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synset('bass.n.07') the member with the lowest range of a family of musical instruments\n",
            "Synset('sea_bass.n.01') the lean flesh of a saltwater fish of the family Serranidae\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Count Vectorizer - Python"
      ],
      "metadata": {
        "id": "KinE9otqPgmR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "corpus = [\n",
        "'This is the first document from heaven',\n",
        "'but the second document is from mars',\n",
        "'And this is the third one from nowhere',\n",
        "'Is this the first document from nowhere?',\n",
        "]\n",
        "\n",
        "df = pd.DataFrame({'Text': corpus})\n",
        "print(df)\n"
      ],
      "metadata": {
        "id": "4qM5OWHpPnwM",
        "outputId": "9a266713-3075-4fb6-b84b-c694d6f7f0b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                       Text\n",
            "0    This is the first document from heaven\n",
            "1      but the second document is from mars\n",
            "2    And this is the third one from nowhere\n",
            "3  Is this the first document from nowhere?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "count_v = CountVectorizer()\n",
        "X = count_v.fit_transform(df.Text).toarray()\n",
        "feature_names = count_v.get_feature_names_out()\n",
        "print(feature_names)"
      ],
      "metadata": {
        "id": "K34PZzehQn1P",
        "outputId": "8d52e4fd-12ee-4838-b026-5a9a3b9d6cb4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['and' 'but' 'document' 'first' 'from' 'heaven' 'is' 'mars' 'nowhere'\n",
            " 'one' 'second' 'the' 'third' 'this']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X)\n",
        "print(count_v.vocabulary_)"
      ],
      "metadata": {
        "id": "QQ2EvAW2SKvQ",
        "outputId": "44997f2a-dd1a-4d9e-9b0e-bd5ab2f6eec9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 1 1 1 1 1 0 0 0 0 1 0 1]\n",
            " [0 1 1 0 1 0 1 1 0 0 1 1 0 0]\n",
            " [1 0 0 0 1 0 1 0 1 1 0 1 1 1]\n",
            " [0 0 1 1 1 0 1 0 1 0 0 1 0 1]]\n",
            "{'this': 13, 'is': 6, 'the': 11, 'first': 3, 'document': 2, 'from': 4, 'heaven': 5, 'but': 1, 'second': 10, 'mars': 7, 'and': 0, 'third': 12, 'one': 9, 'nowhere': 8}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count_v = CountVectorizer(stop_words = ['this','is'])\n",
        "X = count_v.fit_transform(df.Text).toarray()"
      ],
      "metadata": {
        "id": "FfO6Oqt5StLY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X)\n",
        "print(count_v.vocabulary_)"
      ],
      "metadata": {
        "id": "ayv1DJ4hTCBO",
        "outputId": "c0aeeeed-87f0-400f-d4ed-a07587b62b0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 1 1 1 1 0 0 0 0 1 0]\n",
            " [0 1 1 0 1 0 1 0 0 1 1 0]\n",
            " [1 0 0 0 1 0 0 1 1 0 1 1]\n",
            " [0 0 1 1 1 0 0 1 0 0 1 0]]\n",
            "{'the': 10, 'first': 3, 'document': 2, 'from': 4, 'heaven': 5, 'but': 1, 'second': 9, 'mars': 6, 'and': 0, 'third': 11, 'one': 8, 'nowhere': 7}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TF-IDF Vectorizer"
      ],
      "metadata": {
        "id": "CeRCnrtCTlwm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [\n",
        "'This is the first document from heaven',\n",
        "'but the second document is from mars',\n",
        "'And this is the third one from nowhere',\n",
        "'Is this the first document from nowhere?',\n",
        "]\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "vectorizer.fit(corpus)"
      ],
      "metadata": {
        "id": "JiRAj3AGUnoy",
        "outputId": "8d85b4d9-d4c6-4eb5-d568-368eb27a3d8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TfidfVectorizer()"
            ],
            "text/html": [
              "<style>#sk-container-id-7 {color: black;background-color: white;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>TfidfVectorizer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" checked><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print (vectorizer.vocabulary_)"
      ],
      "metadata": {
        "id": "uWHroszHUs23",
        "outputId": "417e98f2-630c-4db7-f50d-c0c083903d39",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'this': 13, 'is': 6, 'the': 11, 'first': 3, 'document': 2, 'from': 4, 'heaven': 5, 'but': 1, 'second': 10, 'mars': 7, 'and': 0, 'third': 12, 'one': 9, 'nowhere': 8}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(vectorizer.idf_)"
      ],
      "metadata": {
        "id": "xWCVtQyUU36n",
        "outputId": "4efe5d0d-0c18-4bfd-c296-abbd9b272f15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.91629073 1.91629073 1.22314355 1.51082562 1.         1.91629073\n",
            " 1.         1.91629073 1.51082562 1.91629073 1.91629073 1.\n",
            " 1.91629073 1.22314355]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hashing Vectorizer"
      ],
      "metadata": {
        "id": "VFu1uP8_VvBJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [\n",
        "'This is the first document from heaven',\n",
        "'but the second document is from mars',\n",
        "'And this is the third one from nowhere',\n",
        "'Is this the first document from nowhere?',\n",
        "]\n",
        "\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame({'Text' :corpus})\n",
        "hash_v = HashingVectorizer(n_features =9 , norm = None , alternate_sign = False)\n",
        "print(hash_v.fit_transform(df.Text).toarray())"
      ],
      "metadata": {
        "id": "FaxWdCmXVzZT",
        "outputId": "8abdaa9f-f7b0-49ca-eebf-cb2fb7741ed9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 1. 2. 2. 1. 0. 1. 0. 0.]\n",
            " [0. 1. 2. 1. 2. 1. 0. 0. 0.]\n",
            " [1. 1. 0. 2. 1. 2. 1. 0. 0.]\n",
            " [0. 1. 2. 2. 1. 0. 1. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hash_v = HashingVectorizer(n_features =15 , norm = None , alternate_sign = False)\n",
        "print(hash_v.fit_transform(df.Text).toarray())"
      ],
      "metadata": {
        "id": "qLeiqeX_X2nG",
        "outputId": "4a352172-ffec-45e7-990b-535b61a4d3d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 1. 2. 0. 0. 0. 2. 0. 0. 0. 1. 1. 0. 0.]\n",
            " [0. 0. 2. 1. 0. 1. 0. 2. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 1. 1. 2. 0. 0. 0. 0. 2. 0. 1.]\n",
            " [0. 0. 1. 1. 0. 0. 0. 2. 0. 0. 0. 1. 2. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hash_v = HashingVectorizer(n_features =15 , norm = None , alternate_sign = True)\n",
        "print(hash_v.fit_transform(df.Text).toarray())"
      ],
      "metadata": {
        "id": "HKtHkL-5X6jw",
        "outputId": "9871db3b-19e2-4127-e977-eb27456291c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.  0. -1.  0.  0.  0.  0.  0.  0.  0.  0. -1.  1.  0.  0.]\n",
            " [ 0.  0.  0. -1.  0. -1.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
            " [ 0.  0.  0. -1.  0. -1.  1.  0.  0.  0.  0.  0.  2.  0. -1.]\n",
            " [ 0.  0. -1. -1.  0.  0.  0.  0.  0.  0.  0. -1.  2.  0.  0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Example's"
      ],
      "metadata": {
        "id": "L_VxsnXOYY5S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spam filter using CountVectorizer"
      ],
      "metadata": {
        "id": "wyg8NZaTYKu2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [\n",
        "'i earn 20 lakh rupees per month just chitchating on the net!' , #spam Mail\n",
        "'are you free for a meeting anytime tomorrow?' # Useful\n",
        "]\n",
        "\n",
        "\n",
        "df = pd.DataFrame({'Text': corpus})\n",
        "print(df)\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "count_v = CountVectorizer()\n",
        "X = count_v.fit_transform(df.Text).toarray()\n",
        "print(X)\n",
        "feature_names = count_v.get_feature_names_out()\n",
        "\n",
        "print(X)\n",
        "print(count_v.vocabulary_)\n"
      ],
      "metadata": {
        "id": "e_D-QNrYYR21",
        "outputId": "0012c4df-2a91-4572-f669-074b369735a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 0 0 1 1 0 0 1 1 0 1 1 1 1 1 1 0 0]\n",
            " [0 1 1 0 0 1 1 0 0 1 0 0 0 0 0 0 1 1]]\n",
            "{'earn': 4, '20': 0, 'lakh': 8, 'rupees': 14, 'per': 13, 'month': 10, 'just': 7, 'chitchating': 3, 'on': 12, 'the': 15, 'net': 11, 'are': 2, 'you': 17, 'free': 6, 'for': 5, 'meeting': 9, 'anytime': 1, 'tomorrow': 16}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# New mail -> same as not spam with new words with exiting count vector\n",
        "new_txt = ['io etrn are you free ruppee for a monnth meeting chitcchting anytime tomorrow neet']\n",
        "df_new = pd.DataFrame({'new_txt':new_txt})\n",
        "y = count_v.transform(df_new.new_txt).toarray()\n",
        "print(y)"
      ],
      "metadata": {
        "id": "U7owM5IteEbA",
        "outputId": "30f9fb23-037d-40de-b92f-b1057893fb3e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 1 1 0 0 1 1 0 0 1 0 0 0 0 0 0 1 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# New Count Vector\n",
        "corpus = [\n",
        "'io etrn are you free ruppee for a monnth meeting chitcchting anytime tomorrow neet'\n",
        "]\n",
        "\n",
        "df = pd.DataFrame({'Text': corpus})\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "count_v = CountVectorizer()\n",
        "X = count_v.fit_transform(df.Text).toarray()\n",
        "feature_names = count_v.get_feature_names_out()\n",
        "print(X)\n"
      ],
      "metadata": {
        "id": "6mdx3PPBjhqO",
        "outputId": "d4c5ced0-2990-465a-c153-a59c66091e43",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 1 1 1 1 1 1 1 1 1 1 1 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We Have to train the model again and again so that span and not span would be classified properly"
      ],
      "metadata": {
        "id": "BWCZEKgwmMR3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spam Filter using Hashing"
      ],
      "metadata": {
        "id": "wr-DLmZvmWCq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [\n",
        "'i earn 20 lakh rupees per month just chitchating on the net!' , #spam Mail\n",
        "'are you free for a meeting anytime tomorrow?' # Useful\n",
        "]\n",
        "\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame({'Text' :corpus})\n",
        "hash_v = HashingVectorizer(n_features =60 , norm = None , alternate_sign = False)\n",
        "print(hash_v.fit_transform(df.Text).toarray())\n"
      ],
      "metadata": {
        "id": "lG8oLmrLmb_7",
        "outputId": "42efb5cd-3126-462b-a7fd-03d94261adc8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 1. 0.\n",
            "  1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = ['io etrn are you free ruppee for a monnth meeting chitcchting anytime tomorrow neet']\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame({'Text' :corpus})\n",
        "hash_v = HashingVectorizer(n_features =60 , norm = None , alternate_sign = False)\n",
        "print(hash_v.fit_transform(df.Text).toarray())\n"
      ],
      "metadata": {
        "id": "QlyP0IjKobQR",
        "outputId": "8e0aae4e-5779-453e-e66c-7ec824867d31",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1.\n",
            "  0. 1. 0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 1. 0. 0. 0. 0. 0. 1. 1. 1. 0.]]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colaboratory",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}